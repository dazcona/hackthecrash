{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, GRU, Dense, MaxPooling1D, Conv1D, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_word_vectors():\n",
    "    \"\"\" Load pre-trained GLoVe vectors \"\"\"\n",
    "\n",
    "    print('[INFO] Loading word vectors...')\n",
    "    embeddings_index = {}\n",
    "    with open('../../data/glove.6B.50d.txt') as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('[INFO] Found {:,} word vectors.'.format(len(embeddings_index)))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train = np.load(os.path.join(data_path, 'X_text_train.npy'), allow_pickle=True)\n",
    "X_text_val = np.load(os.path.join(data_path, 'X_text_val.npy'), allow_pickle=True)\n",
    "y_text_train = np.load(os.path.join(data_path, 'y_text_train.npy'), allow_pickle=True)\n",
    "y_text_val = np.load(os.path.join(data_path, 'y_text_val.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = X_text_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_words = list(set([ word for caption in train_captions for word in caption.split() ]))\n",
    "print('[INFO] {:,} words in the dev captions'.format(len(caption_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZATION\n",
    "\n",
    "print('[INFO] Vectorize the captions into a 2D integer tensor...')\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "# # Save Tokenizer\n",
    "# with open(config.CAPTIONS_EMBEDDINGS_TOKENIZER, 'wb') as f:\n",
    "#     pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "word_index = tokenizer.word_index\n",
    "MAX_NUM_WORDS = len(word_index)\n",
    "MAX_SEQUENCE_LENGTH = max([ len(caption.split()) for caption in train_captions ])\n",
    "# # Save\n",
    "# with open(config.CAPTIONS_MAX_SEQUENCE_LENGTH, 'w') as f:\n",
    "#     f.write(str(MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y TRAIN\n",
    "X_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train = np.array(y_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y validation\n",
    "print('[INFO] Preprocessing validation captions...')\n",
    "val_captions = X_text_val.tolist()\n",
    "validation_sequences = tokenizer.texts_to_sequences(val_captions)\n",
    "X_val = pad_sequences(validation_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_val = np.array(y_text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Loading word vectors')\n",
    "embeddings_index = load_pretrained_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "\n",
    "print('[INFO] Creating the embeddings matrix...')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('[INFO] Embedding Matrix\\'s shape is {}'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_UNITS = 32\n",
    "DROPOUT = 0.5\n",
    "RECURRENT_DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 1\n",
    "DECAY = 1e-3 / NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Training GRU model...')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = GRU(\n",
    "    units=NUM_UNITS, \n",
    "    dropout=DROPOUT, \n",
    "    recurrent_dropout=RECURRENT_DROPOUT,\n",
    "    return_sequences=False,\n",
    ")(embedded_sequences)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.5, seed=42)(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.5, seed=42)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.25, seed=42)(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "print('[INFO] Model\\'s Summary')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE\n",
    "\n",
    "print('[INFO] Compiling model...')\n",
    "\n",
    "# Optimizer\n",
    "opt = Adam(lr=LEARNING_RATE, decay=DECAY)\n",
    "\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=opt,\n",
    "    metrics=['mse', 'mae', 'mape'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT\n",
    "\n",
    "print('[INFO] Fitting model...')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='../../logs')\n",
    "\n",
    "checkpoints = ModelCheckpoint(\n",
    "os.path.join(\n",
    "    '../../checkpoints',\n",
    "    'weights-{epoch:02d}-{val_loss:.10f}.hdf5'),\n",
    "monitor='val_mean_squared_error', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "H = model.fit(X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=NUM_EPOCHS,\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    "    use_multiprocessing=True,\n",
    "    workers=8,\n",
    "    callbacks=[\n",
    "        tensorboard,\n",
    "        checkpoints,\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING LOSS vs ACCURACY\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"mean_squared_error\"], label=\"train_MSE\")\n",
    "plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_mean_squared_error\"], label=\"val_MSE\")\n",
    "plt.title(\"Training Loss and MSE\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/MSE\")\n",
    "plt.legend()\n",
    "plt.savefig('{}/embeddings_loss_vs_MSE.png'.format(config.RUN_LOG_FOLD_DIR.format(fold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Predicting values...')\n",
    "    predicted = model.predict(X_val).flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
